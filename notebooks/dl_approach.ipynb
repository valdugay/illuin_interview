{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d1d1c8",
   "metadata": {},
   "source": [
    "# Transition to Deep Learning\n",
    "\n",
    "After evaluating several classical machine learning models (TF-IDF + Linear Models / SVM / LightGBM), we move to a **Deep Learning approach** to better capture the semantic structure of the data.\n",
    "\n",
    "## Why Deep Learning?\n",
    "\n",
    "Traditional models based on TF-IDF ignore:\n",
    "\n",
    "- word order  \n",
    "- long-range dependencies  \n",
    "- semantic meaning  \n",
    "- similarities between exercises  \n",
    "\n",
    "Although we could use simpler neural architectures such as dense networks or CNNs, these models are **not well suited for sequential textual data**.  \n",
    "\n",
    "Recurrent architectures (RNNs, LSTMs, GRUs) can handle sequences, but they struggle with long texts, lack parallelization, and are generally outperformed by more modern approaches.\n",
    "\n",
    "## Transformers: the right architecture for text\n",
    "\n",
    "Transformers are currently the **state-of-the-art** in natural language processing because they:\n",
    "\n",
    "- handle long sequences  \n",
    "- capture global context with self-attention  \n",
    "- are highly parallelizable  \n",
    "- perform extremely well in multilabel classification  \n",
    "\n",
    "Therefore, Transformers are the most appropriate architecture for our task.\n",
    "\n",
    "## The challenge: limited data & limited compute\n",
    "\n",
    "Training a Transformer from scratch is **not feasible** in our setting:\n",
    "\n",
    "- we do not have enough labeled data  \n",
    "- we cannot train a large model from scratch  \n",
    "- the computational cost would be prohibitive\n",
    "\n",
    "## Solution: Transfer Learning\n",
    "\n",
    "We leverage **pretrained Transformer models** and fine-tune them on our dataset:\n",
    "\n",
    "- we keep the pretrained encoder (frozen or partially frozen)  \n",
    "- we add a **custom multilabel classification head** on top  \n",
    "- we train only the final layers on our dataset  \n",
    "\n",
    "This approach drastically reduces sample complexity and compute requirements.\n",
    "\n",
    "## Suitable pretrained models\n",
    "\n",
    "- **For problem descriptions (natural language):**  \n",
    "  - *DistilBERT*  \n",
    "  - *BERT-base*  \n",
    "  - *RoBERTa-base*\n",
    "\n",
    "- **For source code (programming languages):**  \n",
    "  - *CodeBERT (Microsoft)*  \n",
    "  - *GraphCodeBERT*  \n",
    "  - *CodeT5*\n",
    "\n",
    "These pretrained models already encode meaningful representations of text or code, making them ideal for fine-tuning on our multilabel classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78be3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ae292db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e64b7a",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a415117c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3538342556.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Local Case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_processed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_processed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Local Case\n",
    "\n",
    "from src.processing import load_processed_data\n",
    "\n",
    "df = load_processed_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d23b8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f4ba0987-492d-47bc-8193-38541eef4def\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>src_uid</th>\n",
       "      <th>source_code</th>\n",
       "      <th>tags</th>\n",
       "      <th>full_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bb3fc45f903588baf131016bea175a9f</td>\n",
       "      <td># calculate convex of polygon v.\\n# v is list ...</td>\n",
       "      <td>[geometry]</td>\n",
       "      <td>Problem Description:\\nIahub has drawn a set of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7d6faccc88a6839822fa0c0ec8c00251</td>\n",
       "      <td>s = input().strip();N = len(s)\\nif len(s) == 1...</td>\n",
       "      <td>[strings]</td>\n",
       "      <td>Problem Description:\\nSome time ago Lesha foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>891fabbb6ee8a4969b6f413120f672a8</td>\n",
       "      <td>n = int(input())\\nfor _ in range(n):\\n k,x = m...</td>\n",
       "      <td>[number theory, math]</td>\n",
       "      <td>Problem Description:\\nToday at the lesson of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9d46ae53e6dc8dc54f732ec93a82ded3</td>\n",
       "      <td>temp = list(input())\\nm = int(input())\\ntrans ...</td>\n",
       "      <td>[math, strings]</td>\n",
       "      <td>Problem Description:\\nPasha got a very beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0e0f30521f9f5eb5cff2549cd391da3c</td>\n",
       "      <td>N, B, E = input(), [], 0\\nfor a in map(int, ra...</td>\n",
       "      <td>[math]</td>\n",
       "      <td>Problem Description:\\nYou are given an array $...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4ba0987-492d-47bc-8193-38541eef4def')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f4ba0987-492d-47bc-8193-38541eef4def button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f4ba0987-492d-47bc-8193-38541eef4def');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   index                           src_uid  \\\n",
       "0      0  bb3fc45f903588baf131016bea175a9f   \n",
       "1      1  7d6faccc88a6839822fa0c0ec8c00251   \n",
       "2      2  891fabbb6ee8a4969b6f413120f672a8   \n",
       "3      3  9d46ae53e6dc8dc54f732ec93a82ded3   \n",
       "4      4  0e0f30521f9f5eb5cff2549cd391da3c   \n",
       "\n",
       "                                         source_code                   tags  \\\n",
       "0  # calculate convex of polygon v.\\n# v is list ...             [geometry]   \n",
       "1  s = input().strip();N = len(s)\\nif len(s) == 1...              [strings]   \n",
       "2  n = int(input())\\nfor _ in range(n):\\n k,x = m...  [number theory, math]   \n",
       "3  temp = list(input())\\nm = int(input())\\ntrans ...        [math, strings]   \n",
       "4  N, B, E = input(), [], 0\\nfor a in map(int, ra...                 [math]   \n",
       "\n",
       "                                    full_description  \n",
       "0  Problem Description:\\nIahub has drawn a set of...  \n",
       "1  Problem Description:\\nSome time ago Lesha foun...  \n",
       "2  Problem Description:\\nToday at the lesson of m...  \n",
       "3  Problem Description:\\nPasha got a very beautif...  \n",
       "4  Problem Description:\\nYou are given an array $...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colab Case\n",
    "\n",
    "dataset_url = (\n",
    "    \"https://raw.githubusercontent.com/valdugay/illuin_interview/main/data/processed/cleaned_code_classification_dataset.jsonl\"\n",
    ")\n",
    "\n",
    "df = pd.read_json(dataset_url, lines=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "85c21f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "    \"\"\" Return 8-length binary vectors representing the labels \"\"\"\n",
    "\n",
    "    focus_tags = ['math', 'graphs', 'strings', 'number theory',\n",
    "              'trees', 'geometry', 'games', 'probabilities']\n",
    "\n",
    "    \n",
    "    def encode_tags(tag_list):\n",
    "        return [1 if t in tag_list else 0 for t in focus_tags]\n",
    "\n",
    "    labels_vector = df[\"tags\"].apply(encode_tags)\n",
    "\n",
    "    return np.vstack(labels_vector.values)\n",
    "\n",
    "\n",
    "# To be able to decode the labels later\n",
    "label_mapping = {\n",
    "    'math': 0,\n",
    "    'graphs': 1,\n",
    "    'strings': 2,\n",
    "    'number theory': 3,\n",
    "    'trees': 4,\n",
    "    'geometry': 5,\n",
    "    'games': 6,\n",
    "    'probabilities': 7\n",
    "}\n",
    "\n",
    "\n",
    "Y = get_labels(df)\n",
    "\n",
    "\n",
    "X_descriptions = df[\"full_description\"].values\n",
    "X_code = df[\"source_code\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c665c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as for the ML approach, we have 2 features (text) : the description and the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12339d00",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9cf0695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4cbd14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_mapping)  # 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5145591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_desc = \"distilbert-base-uncased\"\n",
    "tokenizer_desc = AutoTokenizer.from_pretrained(model_name_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1413ddb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 13]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it\n",
    "example_text = \"Given a tree with n nodes, compute the diameter.\"\n",
    "enc = tokenizer_desc(\n",
    "    example_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "enc[\"input_ids\"].shape, enc[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5a654828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_desc = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_desc,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f8e8acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "efc4e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone params: 66,362,880\n",
      "Classifier params: 596,744\n",
      "Total params: 66,959,624\n"
     ]
    }
   ],
   "source": [
    "backbone_params = 0\n",
    "classifier_params = 0\n",
    "\n",
    "for name, param in model_desc.named_parameters():\n",
    "    if \"classifier\" in name:      # Head\n",
    "        classifier_params += param.numel()\n",
    "    else:                         # Pretrained backbone\n",
    "        backbone_params += param.numel()\n",
    "\n",
    "print(f\"Backbone params: {backbone_params:,}\")\n",
    "print(f\"Classifier params: {classifier_params:,}\")\n",
    "print(f\"Total params: {backbone_params + classifier_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3d5b9d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 66,959,624\n"
     ]
    }
   ],
   "source": [
    "# We can check the trainable parameters\n",
    "trainable_params = 0\n",
    "for param in model_desc.parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(f\"Trainable params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d9fd59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags_for_description(texts, model, tokenizer, threshold=0.5, max_length=256):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        probas = torch.sigmoid(logits)\n",
    "        preds = (probas > threshold).int()\n",
    "\n",
    "    return probas, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "db8d4452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " tensor([[0.5026, 0.4769, 0.5000, 0.4561, 0.5085, 0.4643, 0.4819, 0.5361],\n",
      "        [0.5117, 0.4815, 0.4975, 0.4630, 0.5104, 0.4716, 0.4797, 0.5190]])\n",
      "Predictions:\n",
      " tensor([[1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 1, 0, 0, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "texts = [\n",
    "    \"You are given a tree with n vertices. Find the number of paths...\",\n",
    "    \"You are given a string s of length n, consisting of letters 'a' and 'b'...\"\n",
    "]\n",
    "\n",
    "probas, preds = predict_tags_for_description(texts, model_desc, tokenizer_desc)\n",
    "print(\"Probabilities:\\n\", probas)\n",
    "print(\"Predictions:\\n\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "37f7e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DescriptionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0c484c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_desc, X_val_desc, Y_train, Y_val = train_test_split(\n",
    "    X_descriptions, Y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18633042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = DescriptionDataset(X_train_desc, Y_train, tokenizer_desc)\n",
    "val_dataset   = DescriptionDataset(X_val_desc,   Y_val,   tokenizer_desc)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dca25ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model_desc.parameters(), lr=2e-5, weight_decay=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fbff8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU : True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_desc = model_desc.to(device)\n",
    "\n",
    "print(\"GPU :\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "23468140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7eca1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def validate(model, val_loader, criterion=None, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels.float())\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).int().cpu()\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(labels.cpu())\n",
    "\n",
    "    val_loss = total_loss / len(val_loader.dataset)\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    micro = f1_score(all_targets, all_preds, average=\"micro\", zero_division=0)\n",
    "    macro = f1_score(all_targets, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return val_loss, micro, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e46d21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch 1/10 -----\n",
      "➡️ Training ONLY the classification head (backbone frozen).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:08<00:00, 29.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5782\n",
      "Val Micro F1: 0.4614\n",
      "Val Macro F1: 0.0860\n",
      "\n",
      "----- Epoch 2/10 -----\n",
      "➡️ Unfreezing the backbone: training ALL layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:15<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3054\n",
      "Val Micro F1: 0.6426\n",
      "Val Macro F1: 0.3399\n",
      "\n",
      "----- Epoch 3/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:15<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2348\n",
      "Val Micro F1: 0.7076\n",
      "Val Macro F1: 0.5927\n",
      "\n",
      "----- Epoch 4/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:15<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1937\n",
      "Val Micro F1: 0.7290\n",
      "Val Macro F1: 0.6825\n",
      "\n",
      "----- Epoch 5/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:15<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1595\n",
      "Val Micro F1: 0.7274\n",
      "Val Macro F1: 0.6684\n",
      "\n",
      "----- Epoch 6/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:15<00:00, 16.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# A good practice when the dataset is small\n",
    "# First epoch: train only the classification head (freeze the backbone)\n",
    "# From the second epoch: unfreeze the backbone and train all layers\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n----- Epoch {epoch+1}/{epochs} -----\")\n",
    "\n",
    "    if epoch == 0:\n",
    "        print(\"Training ONLY the classification head (backbone frozen).\")\n",
    "        for param in model_desc.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if epoch == 1:\n",
    "        print(\"Unfreezing the backbone: training ALL layers.\")\n",
    "        for param in model_desc.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model_desc.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "    train_loss = train_one_epoch(model_desc, train_loader, optimizer, scheduler)\n",
    "    val_loss, micro, macro = validate(model_desc, val_loader)\n",
    "\n",
    "    test_loss_history.append(val_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Early Stopping (custom) with patience of 2\n",
    "    if len(test_loss_history) > 2 and test_loss_history[-1] > test_loss_history[-2] and test_loss_history[-2] > test_loss_history[-3]:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Micro F1: {micro:.4f}\")\n",
    "    print(f\"Val Macro F1: {macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6b843",
   "metadata": {},
   "source": [
    "# For Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "266a20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# For Code, we need a similar architecture (bert)\n",
    "# But with a model pretrained on code data, like CodeBERT\n",
    "\n",
    "code_model_name = \"microsoft/codebert-base\"\n",
    "\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_model_name)\n",
    "\n",
    "code_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    code_model_name,\n",
    "    num_labels=8,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8218748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_code_train, X_code_val, Y_train, Y_val = train_test_split(\n",
    "    X_code, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "code_train_dataset = DescriptionDataset(X_code_train, Y_train, code_tokenizer, max_length=256)\n",
    "code_val_dataset   = DescriptionDataset(X_code_val,   Y_val,   code_tokenizer, max_length=256)\n",
    "\n",
    "code_train_loader = DataLoader(code_train_dataset, batch_size=8, shuffle=True)\n",
    "code_val_loader   = DataLoader(code_val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eb5a2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_model.to(device)\n",
    "code_optimizer = AdamW(code_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "21afd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch 1/10 -----\n",
      "Training ONLY the classification head (backbone frozen).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:12<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6790\n",
      "Val Loss: 0.6791\n",
      "Val Micro F1: 0.3315\n",
      "Val Macro F1: 0.1715\n",
      "\n",
      "----- Epoch 2/10 -----\n",
      "Unfreezing the backbone: training ALL layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3765\n",
      "Val Loss: 0.3232\n",
      "Val Micro F1: 0.5654\n",
      "Val Macro F1: 0.1925\n",
      "\n",
      "----- Epoch 3/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3040\n",
      "Val Loss: 0.2787\n",
      "Val Micro F1: 0.6427\n",
      "Val Macro F1: 0.3344\n",
      "\n",
      "----- Epoch 4/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2771\n",
      "Val Loss: 0.2617\n",
      "Val Micro F1: 0.6644\n",
      "Val Macro F1: 0.3639\n",
      "\n",
      "----- Epoch 5/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2582\n",
      "Val Loss: 0.2684\n",
      "Val Micro F1: 0.6304\n",
      "Val Macro F1: 0.3657\n",
      "\n",
      "----- Epoch 6/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2353\n",
      "Val Loss: 0.2570\n",
      "Val Micro F1: 0.6656\n",
      "Val Macro F1: 0.4276\n",
      "\n",
      "----- Epoch 7/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2137\n",
      "Val Loss: 0.2551\n",
      "Val Micro F1: 0.6639\n",
      "Val Macro F1: 0.4768\n",
      "\n",
      "----- Epoch 8/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1920\n",
      "Val Loss: 0.2767\n",
      "Val Micro F1: 0.6651\n",
      "Val Macro F1: 0.5015\n",
      "\n",
      "----- Epoch 9/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1709\n",
      "Val Loss: 0.2690\n",
      "Val Micro F1: 0.6794\n",
      "Val Macro F1: 0.5160\n",
      "\n",
      "----- Epoch 10/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:26<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1515\n",
      "Val Loss: 0.2757\n",
      "Val Micro F1: 0.6892\n",
      "Val Macro F1: 0.5533\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n----- Epoch {epoch+1}/{epochs} -----\")\n",
    "\n",
    "    if epoch == 0:\n",
    "        print(\"Training ONLY the classification head (backbone frozen).\")\n",
    "        for param in code_model.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if epoch == 1:\n",
    "        print(\"Unfreezing the backbone: training ALL layers.\")\n",
    "        for param in code_model.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        optimizer = torch.optim.AdamW(code_model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "\n",
    "    train_loss = train_one_epoch(code_model, train_loader, optimizer, scheduler)\n",
    "    val_loss, micro, macro = validate(code_model, val_loader)\n",
    "\n",
    "    test_loss_history.append(val_loss)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Early Stopping (custom) with patience of 2\n",
    "    if len(test_loss_history) > 2 and test_loss_history[-1] > test_loss_history[-2] and test_loss_history[-2] > test_loss_history[-3]:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Micro F1: {micro:.4f}\")\n",
    "    print(f\"Val Macro F1: {macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For code, the results are largely better than ML approach\n",
    "# Let's try an hybrid approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea74d9",
   "metadata": {},
   "source": [
    "# Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0587b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to the architecture of this approach\n",
    "# We won't just 'mix' the preidction (or probabilities) of both models\n",
    "# Instead, we will concatenate the embeddings from both models\n",
    "# And then, use one final classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "26819112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do this, we create a custom model :\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "class MultiEncoderClassifier(nn.Module):\n",
    "    def __init__(self, model_name_1=\"microsoft/codebert-base\", \n",
    "                       model_name_2=\"distilbert-base-uncased\",\n",
    "                       num_labels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder 1: CodeBERT\n",
    "        self.encoder1 = AutoModel.from_pretrained(model_name_1)\n",
    "\n",
    "        # Encoder 2: DistilBERT\n",
    "        self.encoder2 = AutoModel.from_pretrained(model_name_2)\n",
    "\n",
    "        # Hidden sizes\n",
    "        h1 = self.encoder1.config.hidden_size\n",
    "        h2 = self.encoder2.config.hidden_size\n",
    "        combined = h1 + h2\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1,\n",
    "                      input_ids_2, attention_mask_2):\n",
    "\n",
    "        # CodeBERT encoding\n",
    "        out1 = self.encoder1(input_ids=input_ids_1,\n",
    "                             attention_mask=attention_mask_1)\n",
    "        cls1 = out1.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # DistilBERT encoding\n",
    "        out2 = self.encoder2(input_ids=input_ids_2,\n",
    "                             attention_mask=attention_mask_2)\n",
    "        cls2 = out2.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([cls1, cls2], dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "47be6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer1, tokenizer2, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tok1 = tokenizer1\n",
    "        self.tok2 = tokenizer2\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        enc1 = self.tok1(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        enc2 = self.tok2(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids_1\": enc1[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_1\": enc1[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_2\": enc2[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_2\": enc2[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bc7ce3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_dataset = DualEncoderDataset(X_train_desc, Y_train, tokenizer1, tokenizer2)\n",
    "val_dataset   = DualEncoderDataset(X_val_desc,   Y_val,   tokenizer1, tokenizer2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c745960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MultiEncoderClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c686cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d165a26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c3bc4acb2046cc84a7c16c00bacb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 0.3740 | Val Loss = 0.2725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8901132c98a14607acec519717d8324c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss = 0.2605 | Val Loss = 0.2322\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0f3f363c244c6c9d1db104b5df1feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss = 0.2117 | Val Loss = 0.2143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aab0f9096f94ea8a9f5b18664b46a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss = 0.1753 | Val Loss = 0.2180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0937c0548b8c476492bd9d4a03bf2c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    # Train for each batch of training data\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(\n",
    "            batch[\"input_ids_1\"].to(device),\n",
    "            batch[\"attention_mask_1\"].to(device),\n",
    "            batch[\"input_ids_2\"].to(device),\n",
    "            batch[\"attention_mask_2\"].to(device),\n",
    "        )\n",
    "\n",
    "        loss = criterion(logits, batch[\"labels\"].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            logits = model(\n",
    "                batch[\"input_ids_1\"].to(device),\n",
    "                batch[\"attention_mask_1\"].to(device),\n",
    "                batch[\"input_ids_2\"].to(device),\n",
    "                batch[\"attention_mask_2\"].to(device),\n",
    "            )\n",
    "            loss = criterion(logits, batch[\"labels\"].to(device))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss_history.append(total_loss / len(train_loader))\n",
    "    val_loss_history.append(val_loss / len(val_loader))\n",
    "\n",
    "    # Early Stopping (custom) with patience of 2\n",
    "    if len(val_loss_history) > 2 and val_loss_history[-1] > val_loss_history[-2] and val_loss_history[-2] > val_loss_history[-3]:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss = {total_loss/len(train_loader):.4f} | Val Loss = {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "74d487e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Micro F1: 0.7492\n",
      "Val Macro F1: 0.7046\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with our metrics\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "for batch in val_loader:\n",
    "    logits = model(\n",
    "        batch[\"input_ids_1\"].to(device),\n",
    "        batch[\"attention_mask_1\"].to(device),\n",
    "        batch[\"input_ids_2\"].to(device),\n",
    "        batch[\"attention_mask_2\"].to(device),\n",
    "    )\n",
    "    preds = (torch.sigmoid(logits) > 0.5).int().cpu()\n",
    "    y_preds.append(preds)\n",
    "    y_trues.append(batch[\"labels\"].cpu())\n",
    "\n",
    "y_preds = torch.cat(y_preds, dim=0)\n",
    "y_trues = torch.cat(y_trues, dim=0)\n",
    "\n",
    "micro = f1_score(y_trues, y_preds, average=\"micro\", zero_division=0)\n",
    "macro = f1_score(y_trues, y_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Val Micro F1: {micro:.4f}\")\n",
    "print(f\"Val Macro F1: {macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b51843cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "         math       0.81      0.84      0.82       281\n",
      "       graphs       0.76      0.43      0.55       110\n",
      "      strings       0.88      0.83      0.86        90\n",
      "number theory       0.69      0.45      0.54        76\n",
      "        trees       0.86      0.72      0.78        60\n",
      "     geometry       0.68      0.76      0.71        33\n",
      "        games       0.76      0.73      0.74        22\n",
      "probabilities       0.83      0.50      0.62        10\n",
      "\n",
      "    micro avg       0.80      0.71      0.75       682\n",
      "    macro avg       0.78      0.66      0.70       682\n",
      " weighted avg       0.79      0.71      0.74       682\n",
      "  samples avg       0.78      0.74      0.74       682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_trues, y_preds, target_names=label_mapping.keys(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the best result we get through the deep learning approach\n",
    "\n",
    "# The combination of both code and description embeddings seems to help a lot\n",
    "# But we did not improve if we compare with ML approach\n",
    "# While the model is much more complex and not interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2027edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"multi_encoder_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fd3f6a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_eef5d07c-966d-43bf-be5e-18b4eb253b2a\", \"multi_encoder_model.pt\", 772573873)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO save it in local (we trained it in Colab)\n",
    "from google.colab import files\n",
    "files.download(\"multi_encoder_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24805cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'multi_encoder_model.pt', 'sample_data']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ed074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
